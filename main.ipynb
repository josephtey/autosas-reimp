{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('word2vec_model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "doc2vec_model = Doc2Vec.load(\"doc2vec_model/doc2vec_wiki_d300_n5_w8_mc50_t12_e10_dbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams, pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def generate_ngram_results(set_ids, df, count_threshold = 3):\n",
    "    # Initialize dictionary to store results\n",
    "    ngram_results = {i: [] for i in set_ids}\n",
    "\n",
    "    # Loop through each EssaySet\n",
    "    for set_id in set_ids:\n",
    "        # Filter DataFrame for current EssaySet and score1 == 2\n",
    "        filtered_df = df[(df['EssaySet'] == set_id) & (df['Score1'] == 2)]\n",
    "        \n",
    "        # Print progress for each EssaySet\n",
    "        print(f\"Processing EssaySet {set_id} with {len(filtered_df)} essays.\")\n",
    "        \n",
    "        # Initialize a Counter to count n-grams across all essays in the set\n",
    "        ngram_counter = Counter()\n",
    "        \n",
    "        # Process each EssayText in the filtered_df DataFrame\n",
    "        for essay in filtered_df['EssayText']:\n",
    "            tokens = word_tokenize(essay)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            \n",
    "            # Generate bi-grams, tri-grams, and tetra-grams\n",
    "            bi_grams = list(ngrams(pos_tags, 2))\n",
    "            tri_grams = list(ngrams(pos_tags, 3))\n",
    "            tetra_grams = list(ngrams(pos_tags, 4))\n",
    "            \n",
    "            # Concatenate all n-grams into a single list and update the counter\n",
    "            all_ngrams = bi_grams + tri_grams + tetra_grams\n",
    "            ngram_counter.update(all_ngrams)\n",
    "        \n",
    "        # Filter n-grams that appeared at least 3 times\n",
    "        frequent_ngrams = [ngram for ngram, count in ngram_counter.items() if count >= count_threshold]\n",
    "        \n",
    "        # Store the frequent n-grams in the results dictionary\n",
    "        ngram_results[set_id] = frequent_ngrams\n",
    "        \n",
    "        # Print after processing each set\n",
    "        print(f\"Completed processing EssaySet {set_id}, with {len(frequent_ngrams)} n-grams.\")\n",
    "    \n",
    "    return ngram_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EssaySet 3 with 699 essays.\n",
      "Completed processing EssaySet 3, with 403 n-grams.\n"
     ]
    }
   ],
   "source": [
    "ngram_results = generate_ngram_results([3], df, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_prompts(set_ids):\n",
    "    prompts = {}\n",
    "    for set_id in set_ids:\n",
    "        file_name = f\"prompts/asap_{set_id:02d}.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, 'r') as file:\n",
    "                prompts[set_id] = file.read().strip()\n",
    "        else:\n",
    "            print(f\"Prompt file {file_name} not found.\")\n",
    "    return prompts\n",
    "\n",
    "set_ids = list(range(1, 11))  # Example set ids, you can modify this as needed\n",
    "prompts = load_prompts(set_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.feature_extractor import FeatureExtractor\n",
    "\n",
    "feature_extractor = FeatureExtractor(model, doc2vec_model, ngram_results, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "\n",
    "# run this for everythting ... so i can just do it once, and then save it as a csv to re-load ... \n",
    "def add_features_to_df(df, feature_extractor, essay_set, add_word2vec=False, add_doc2vec=False, add_pos=False, add_prompt_overlap=False):\n",
    "    if add_word2vec:\n",
    "        df['word2vec_features'] = None\n",
    "    if add_doc2vec:\n",
    "        df['doc2vec_features'] = None\n",
    "    if add_pos:\n",
    "        df['pos_features'] = None\n",
    "    if add_prompt_overlap:\n",
    "        df['prompt_overlap_features'] = None\n",
    "\n",
    "    total_items = len(df)\n",
    "    print(f\"Processing {total_items} items...\")\n",
    "\n",
    "    # Iterate over each row in the training DataFrame\n",
    "    num = 0\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract features using the feature_extractor object\n",
    "        if add_word2vec:\n",
    "            word2vec_features = feature_extractor.word2vec(row['EssayText'])\n",
    "            df.at[index, 'word2vec_features'] = word2vec_features\n",
    "        if add_doc2vec:\n",
    "            doc2vec_features = feature_extractor.doc2vec(row['EssayText'])\n",
    "            df.at[index, 'doc2vec_features'] = doc2vec_features\n",
    "        if add_pos:\n",
    "            pos_features = feature_extractor.pos(row['EssaySet'], row['EssayText'])\n",
    "            df.at[index, 'pos_features'] = pos_features\n",
    "        if add_prompt_overlap:\n",
    "            prompt_overlap_features = feature_extractor.prompt_overlap(row['EssaySet'], row['EssayText'])\n",
    "            df.at[index, 'prompt_overlap_features'] = prompt_overlap_features\n",
    "\n",
    "        # Update after every 1000 items\n",
    "        if (num) % 1000 == 0:\n",
    "            print(f\"Processed {num} items.\")\n",
    "        num += 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_set = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_set_df = df[df['EssaySet'] == essay_set].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3152 items...\n",
      "Processed 0 items.\n",
      "Processed 1000 items.\n",
      "Processed 2000 items.\n",
      "Processed 3000 items.\n"
     ]
    }
   ],
   "source": [
    "# Save the updated DataFrame to a CSV file\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Execute the function to add features to the DataFrame and display the updated DataFrame\n",
    "add_features_to_df(essay_set_df, feature_extractor, 3, add_doc2vec=True)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists('updated_features'):\n",
    "    os.makedirs('updated_features')\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save the updated DataFrame to a CSV file with the timestamp\n",
    "essay_set_df.to_csv(f'updated_features/updated_features_{timestamp}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data set \n",
    "import numpy as np\n",
    "\n",
    "training_data = essay_set_df[(essay_set_df['DataSet'] == 'Train')].copy(deep=True)\n",
    "training_data = training_data.dropna(subset=['word2vec_features', 'doc2vec_features'])\n",
    "X_train = np.vstack(training_data[['word2vec_features', 'doc2vec_features', 'pos_features', 'prompt_overlap_features']].apply(lambda x: np.hstack(x), axis=1).values)\n",
    "y_train = training_data['Score1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model; hyper-parameter search\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {'n_estimators': randint(50,500),\n",
    "              'max_depth': randint(1,20)}\n",
    "clf = RandomForestClassifier()\n",
    "rand_search = RandomizedSearchCV(clf, \n",
    "                                param_distributions = param_dist, \n",
    "                                n_iter=5, \n",
    "                                cv=5)\n",
    "                                \n",
    "\n",
    "rand_search.fit(X_train, y_train)\n",
    "best_clf = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster, more estimates\n",
    "clf = RandomForestClassifier(max_depth=100, n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "best_clf = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 24 123   2]\n",
      " [ 14 338   8]\n",
      " [  6  98  18]]\n",
      "Cohen Kappa Score (Weighted): 0.1950373198372678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "val_set = essay_set_df[(essay_set_df['DataSet'] == 'Priva')].copy(deep=True)\n",
    "val_set = val_set.dropna(subset=['word2vec_features', 'doc2vec_features'])\n",
    "\n",
    "X_val = np.vstack(val_set[['word2vec_features'  , 'doc2vec_features', 'pos_features', 'prompt_overlap_features']].apply(lambda x: np.hstack(x), axis=1).values)\n",
    "y_val = val_set['Score1']\n",
    "\n",
    "y_pred = best_clf.predict(X_val)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "kappa_score = cohen_kappa_score(y_val, y_pred, weights='quadratic')\n",
    "print(f\"Cohen Kappa Score (Weighted): {kappa_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
